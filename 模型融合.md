# 模型融合

## blending

## stacking

The basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error.

就是base 几个 model 上面挂个 lr之类的。

Let’s say you want to do 2-fold stacking:

-  Split the train set in 2 parts: train_a and train_b
-  Fit a first-stage model on train_a and create predictions for train_b
-  Fit the same model on train_b and create predictions for train_a
-  Finally fit the model on the entire train set and create predictions for the test set.
-  Now train a second-stage stacker model on the probabilities from the first-stage model(s).

一个基本的步骤

​	

## bagging(随机森林)

#### bagging

ranking bagging

说白了是因为predict 的值差距较小，做 ensemble 没反应。先做 normalize，再 bag。

# 







## boosting(gbdt)

## Online Stacking